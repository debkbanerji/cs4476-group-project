<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css"
          integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">
    <link rel="shortcut icon" type="image/png" href="assets/images/favicon.ico"/>
    <title>Final Project Update</title>
</head>
<body>
<div>
    <div class="row" style="padding: 0; overflow: hidden; width: 98vw">
        <div class="col-xs-12" style="width: 100%">
            <div class="container-fluid" style="text-align: center; width: 100%">
                <h1>Final Project Update - The Seam Team</h1>
                <h2>CS 4476 - Computer Vision</h2>
                <span>Team members:
                    <a href="mailto:dbanerji3@gatech.edu">Deb Banerji</a>,
                    <a href="mailto:chsieh40@gatech.edu">Christine Hsieh</a>,
                    <a href="mailto:Kchen357@gatech.edu">Kevin Chen</a>,
                    <a href="mailto:Kpatel349@gatech.edu">Kirtan Patel</a>,
                    <a href="mailto:rkrishnan42@gatech.edu">Rohith Krishnan</a>,
                    <a href="mailto:smasand6@gatech.edu">Suraj Masand</a>
                </span>
            </div>
        </div>
        <div class="col-sm-12">
            <div class="container-fluid">
                <br>
                <h2>Abstract</h2>
                <hr>
                <p>
                  As more people opt to utilize the convenience and speed of shopping online, the problem of determining clothing fit becomes more and more apparent and important to solve. If online shoppers were able to see what a particular item of clothing looks like on them, we predict they may be more likely to complete their purchase. Therefore, we propose a system that will take an article of clothing and map it onto an image of the user, thus allowing users to more confidently decide if an item of clothing “looks good” on them. Our system is simple as it utilizes homography transformations and seam carving to fit the article of clothing onto the user. We ran our system on just T-shirts and our results give a fairly accurate idea of the fit of the t-shirt on a potential consumer.
                </p>
            </div>
            <div class="container-fluid">
                <br>
                <h2> Teaser Image </h2>
                <hr>
                <img src="assets/images/teaserImage.png" class="img-fluid" style="width: 75%">

                <p><br><i>This image demonstrates the basic idea behind our system. After inputting an image of themselves and the shirt that they are looking to buy, users will click a few correspondence points and then be able to see what that shirt looks like overlaid on their body. While this image was created using traditional photo-editing techniques of cropping and manual shrinking, this is time consuming and doesn't always result in an accurate picture. Our system will automate this process and give the user a better idea of what they look like in the indicated clothing item.</i></p>

            </div>
        </div>
        <div class="col-sm-12">
            <div class="container-fluid">
                <br>
                <h2>Introduction</h2>
                <hr>
                <p>
                  When people shop for clothing items online, it is often hard for them to tell how well an item of clothing will fit on themselves. Oftentimes, the model that is used to display the clothing item represents an “idea” physique for that article of clothing, making it difficult for the user to image how the item will fit on their own physique. There have been attempts to solve this problem by attempting to “fit” the image of the clothing item onto an image or 3D model of the user. These attempts have shown success but require complex 3D models [1] or an expensive camera setup that will track the user’s pose and physique [2]. Our solution removes the cost and complexity of fitting a clothing item to the user’s body while still giving an accurate representation of how an article of clothing will fit onto the user. The Seam Team presents a method that allows the everyday online-shopper to take an image of a piece of clothing and overlay it onto an image of themselves so they can have a realistic idea of how something fits before making a purchase.
                </p>
            </div>
        </div>
        <div class="col-sm-12">
            <div class="container-fluid">
                <h2>Approach</h2>
                <hr>


                <h4> Collecting Correspondence Points</h4>
                <p>
                    To be able to transform the image of the t-shirt onto the image of the user, we need to collect
                    correspondence information between common points on the t-shirt image and on the user’s body. Our
                    system first requests an image of the user in the predefined pose we outlined in our project
                    proposal. Once the image is uploaded we provide the user with an example image of a shirt with the 10 key points we need them to select on their own image. These are the points that will be found on both the image of the user and images of their chosen t-shirts to generate the homography mappings. The image below shows the 10 points we ask the user to manually select:
                </p>

                <div class="container-fluid">
                    <img src="assets/images/referenceShirt.png" class="img-fluid" style="width: 50%">
                    <p><i>(Shirt Reference for selecting Points.)</i></p>
                </div>

                <p>
                    In order for the homography matrix to be calculated correctly, we originally prompted the user to identify the same 10 points above in every shirt image that they wish to see themselves in. This approach required the user to click a total of 20 different points in a specific order in order to get a proper output for a single shirt mapping (this approach was presented in the midterm update). However, this tedious task can very easily disuade individuals from using our system. Therefore, in order to reduce the work of the user, for all other t-shirt images, we have used the automatic corner detection and labelling (explained in the following section) to identify the corresponding points. The uploaded image of the user, the corresponding points in the t-shirt image, and the corresponding points in the image of the user, are all saved as npy files to be used by the other parts of the program.
                </p>


                <h4> Intelligent Corner Detection and Labeling</h4>
                <p>
                    In order to enable maximum convenience and ease of use, we wanted our system to intelligently detect important points on images of shirts. Furthermore, it would have to successfully identify and label them according to which part of the shirt they represent. By successfully identifying points such as ‘top point of right sleeve’ or ‘bottom most corner of shirt’ the system would be able to use this information to intelligently map points for homography between images with a high level of accuracy, avoiding potential mismatches. After experimenting with different techniques, we determined that due to the nature of the images we’d be working with, no single technique would be able to reliably detect and identify all of the different types of points with a high enough degree of accuracy for our purposes. Therefore, we used a combination of multiple techniques in order to achieve the desired result, leading to a detecting and labelling system that was reliable, flexible and robust to errors.
                </p>

                <h5>Harris Corner Detection and ‘Outside-In’ Labelling</h5>
                <p>
                  The first technique that we used in order to identify important points was dependent on Harris Corner detection. In order to use this corner detection technique, we first isolated the foreground of the image, essentially picking out each shirt from the background, as can be seen in the examples below.
                </p>
                <br>

                <div>
                  <img src="assets/images/backgroundIm1.png"  alt="Corner Detector 1" style="width: 75%">
                  <img src="assets/images/backgroundIm2.png" alt="Corner Detector 2" style="width: 75%">
                </div>

                <div class="container-fluid">
                    <p><i>(Examples of Background Identification in sample shirt images.)</i></p>
                </div>

                <p>
                  From these examples, one can see the isolated foreground (yellow) separated from the background (purple). By using hole filling, the algorithm used for isolating the foreground is able to successfully pick out the entire shirt, despite certain patches of the shirt possibly being the same or of similar color as the background. (such as the white patches in the grey polo from the second example). This identification of the shirt apart from the background of the image is also used in generating the homography of the shirt onto the user.

                  Harris corner detection can then use this extracted foreground in order to isolate a subset of the target corners from the image. The value of ‘k’ used for the corner detection algorithm is important, as it dictates how sensitive it is. We used a binary-search like procedure in order to test different values and settled upon 0.05. Using other values of k would either cause important corners to not be detected, or too many corners to be detected. (this would make it much harder to correctly label the required corners).
                </p>

                <p> <b>Key for points labelled in images:</b></p>
                <li style="margin-left: 2em"> lSTC: left sleeve top corner </li>
                <li style="margin-left: 2em">lSBC: left sleeve bottom corner </li>
                <li style="margin-left: 2em">bLC: bottom left corner </li>
                <li style="margin-left: 2em">rSBC: right sleeve bottom corner </li>
                <li style="margin-left: 2em">rSTC: right sleeve top corner </li>
                <li style="margin-left: 2em">bRC: bottom right corner </li>
                <br>

                <div>
                  <img src="assets/images/cornerDetector1.png"  alt="Corner Detector 1" style="width: 75%">
                  <img src="assets/images/cornerDetector2.png" alt="Corner Detector 2" style="width: 75%">
                </div>

                <div class="container-fluid">
                    <p><i>(Shirt Images with Detected Corners.)</i></p>
                </div>

                <p>
                    Once the system runs Harris corner detection, it labels relevant detected corners so they can be used as correlation points across different images. Here, we use what we call ‘outside-in’ labelling - the system labels outer corners first. It does this by making inferences about what they represent, based on their position in the image, which it can do with a high degree of accuracy, as they are at relatively extreme points of the image. After labelling the outer corners with a relatively high degree of accuracy the system can then use this information to label inner corners based on their positions relative to the outer corners. Using this approach, a portion of the points can be successfully labelled with a high degree of accuracy.
                </p>

                <h5> Limits of Harris Corner Detection </h5>
                <p>
                  Unfortunately, Harris corner detection is limited in that it cannot detect points of interest marked by more subtle curves. This is an issue in a large number of images when trying to detect the shoulders or neck points of shirts. Even turning up the sensitivity by adjusting the value of k does not help detect these points, much less label them. Therefore, we needed to incorporate another approach and combine its results with those of Harris corner detection.
                </p>

                <h5> A Linear Programming Based Approach </h5>
                <p>
                  In order to detect and label the relevant neck and shoulder points, we borrow a trick from linear programming. More specifically, we visualize the problem of detecting points as similar to optimizing a linear objective function in a two dimensional plane. We use a weighting of x and y in order to define the direction along which this function is most sharply increasing - by choosing the correct x and y values for each target point, we can detect and label the neck and shoulder points with a high degree of accuracy. The ‘constraint’ of this optimization problem is that these points must lie within the foreground (in practice, they will always be at the boundary of the foreground and the background).

                  Implementation wise, we experimented with breaking down the boundary into a variety of different functions, since the constraint is clearly not a linear one. Unfortunately, constructing these functions turned out to be complex and inefficient in practice. Instead, we took advantage of the fact that since domain of each function was discrete (the coordinates of pixels in the image itself) it was feasible to calculate the objective function for each pixel. In practice, due to the simplicity of the objective function (the objective function was linear), the low overhead of evaluating this function meant that this part of the algorithm ran quite quickly.

                </p>

                <p> <b>Key for points labelled in images:</b></p>
                <li style="margin-left: 2em"> lSC: Left Shoulder Corner </li>
                <li style="margin-left: 2em"> lNC: Left Neck Corner </li>
                <li style="margin-left: 2em"> rSC: Right Shoulder Corner </li>
                <li style="margin-left: 2em"> rNC: Right Neck Corner </li>

                <div class="container-fluid">
                    <img src="assets/images/labeledCorners1.png" class="img-fluid" style="width: 75%">
                    <p><i>(Labeled Corners using Linear Programming.)</i></p>
                </div>

                <h5> Full Detection and Labelling System Test Cases </h5>

                <p>
                  Here are some examples of the entire detection and labelling system, using both Harris corner detection and the linear programming based approach
                </p>

                <p> <b>Key for points labelled in images:</b></p>
                <li style="margin-left: 2em"> lSTC: left sleeve top corner </li>
                <li style="margin-left: 2em"> lSBC: left sleeve bottom corner </li>
                <li style="margin-left: 2em"> bLC: bottom left corner </li>
                <li style="margin-left: 2em"> rSBC: right sleeve bottom corner </li>
                <li style="margin-left: 2em"> rSTC: right sleeve top corner </li>
                <li style="margin-left: 2em"> bRC: bottom right corner </li>
                <li style="margin-left: 2em"> lSC: Left Shoulder Corner </li>
                <li style="margin-left: 2em"> lNC: Left Neck Corner </li>
                <li style="margin-left: 2em"> rSC: Right Shoulder Corner </li>
                <li style="margin-left: 2em"> rNC: Right Neck Corner </li>
                <br>

                <div id="myCarousel3" class="carousel slide" data-ride="carousel" style="width: 50%; margin: auto">
                    <!-- Indicators -->
                    <ol class="carousel-indicators">
                      <li data-target="#myCarousel3" data-slide-to="0" class="active"></li>
                      <li data-target="#myCarousel3" data-slide-to="1"></li>
                      <li data-target="#myCarousel3" data-slide-to="2"></li>
                      <li data-target="#myCarousel3" data-slide-to="3"></li>
                      <li data-target="#myCarousel3" data-slide-to="4"></li>
                    </ol>
                    <!-- Wrapper for slides -->
                    <div class="carousel-inner">
                      <div class="item active">
                        <img src="assets/images/fullDetection1.png" alt="Full Detection" style="height: 500px; margin: auto">
                      </div>

                      <div class="item">
                        <img src="assets/images/fullDetection2.png" alt="Full Detection" style="height: 500px; margin: auto">
                      </div>

                      <div class="item">
                        <img src="assets/images/fullDetection3.png" alt="Full Detection" style="height: 500px; margin: auto">
                      </div>
                      <div class="item">
                        <img src="assets/images/fullDetection4.png" alt="Full Detection" style="height: 500px; margin: auto">
                      </div>
                      <div class="item">
                        <img src="assets/images/fullDetection5.png" alt="Full Detection" style="height: 500px; margin: auto">
                      </div>
                    </div>
                    <!-- Left and right controls -->
                    <a class="left carousel-control" href="#myCarousel3" data-slide="prev">
                      <span class="glyphicon glyphicon-chevron-left"></span>
                      <span class="sr-only">Previous</span>
                    </a>
                    <a class="right carousel-control" href="#myCarousel3" data-slide="next">
                      <span class="glyphicon glyphicon-chevron-right"></span>
                      <span class="sr-only">Next</span>
                    </a>
                </div>

                  <div class="container-fluid" style="text-align: center;">
                    <p><i>(Examples of an entirely, accurately labelled t-shirt image.)</i></p>
                  </div>


                <h4> Collar Detection </h4>
                <p>
                  Once these are correspondence points are calculated, we can approximate the area around the back of the collar where the user’s neck would be and “cut out” a section of the collar in the foreground mask. We do this by taking the two collar points (1 and 10 in the image above) and assume that to be the diameter of a circle where the collar is. Then we set any points within that circle as the background so that the back of the collar area will not appear in the final image transformation. With the updated foreground mask, the resulting image should have a more natural-looking collar around the neck. Once the foreground mask is updated to crop out the back of the collar of the shirt, it is saved as an npy to be used later in the program.
                </p>


                <h4> Seam Carving </h4>
                <p>
                    Once the correspondance points have been determined, we use seam carving to avoid strange distortions in the shirt's pattern/text when the homography is applied. In order to avoid
                    these distortions the resizing of the shirt must be completed with the intent of making the aspect
                    ratio of the shirt comparable to the aspect ratio of the example shirt provided in the input image
                    of the user. By resizing the shirt to have similar width-to-height proportions as the figure of the
                    user, we have reduced opportunities for a shirt’s logo or image to be distorted. In order to
                    complete this task, we used certain correspondence points to determine the approximate width and height of the shirt.
                </p>

                <div class="container-fluid">
                    <img src="assets/images/sizeDefinition.png" class="img-fluid" style="width: 25%">
                    <p><i>(Height was defined as the number of pixels from the collar line of the shirt to the bottom of the shirt. Width was defined as the number of pixels between each side of the torso. These values were determined using the correspondence points provided by the user in the initial set up stage.)</i></p>
                </div>
                <p>
                  Once the aspect ratio of the shirt and the user was determined, it was a matter of applying basic algebraic manipulation to calculate the necessary number of pixels that must be added/removed horizontally or vertically to the shirt. The interesting part of this process was determining how to pick which direction of seam carving would be better: manipulating the width or the height. For now, we calculate both possible manipulations to the shirt and pick the change that results in the least number of seams that must be added/removed. As discussed in the midterm report, we were needed to create a smarter seam carving algorithm than the one proposed in [3] to account for the unique properties of seam carving a shirt. We needed to make sure the seam carving wouldn't choose to add/remove seams in the background of the shirt image, and we needed to ensure that the seam carving wouldn't distort the shape of the shirt.
                </p>

                <div class="container-fluid">
                    <img src="assets/images/seamCarvingSuccess.png" class="img-fluid" style="width: 48%">
                    <img src="assets/images/seamCarvingFail.png" class="img-fluid" style="width: 48%">
                    <p><i>(Two failure cases we needed to consider when modifying the seam carving algorithm to work for modifying t-shirts.)</i></p>
                </div>

                <p>
                    Additionally, in order to allow for cleaner code and a faster runtime, we reimplemented all of the seam carving
                    steps from scratch, altering them in order to accomodate our new approach. This new implementation
                    creates containers large enough to work with the larger of the input and final images before
                    carrying out the steps of the algorithm. This allows all intermediate steps to be efficiently
                    carried out in place. Instead of copying the entire image to a new array during each seam change,
                    efficient pixel shifting is employed instead. The same is done for the corresponding energy image,
                    which is also updated on the fly so it does not need to be calculated. The result is that even
                    though our algorithm should theoretically take longer to run due to the more complex dynamic
                    programming procedure, in practice it is quite fast when compared to our regular seam carving
                    implementations from earlier in the semester. For the given example (the black shirt) our new
                    implementation was approximately five times as fast as both of the problem set 2 implementations we
                    tested it against, despite the more complicated algorithm. This increase in speed was very useful to
                    us, since it allowed us to test on a larger amount of data within a reasonable amount of time.
                </p>
                <br>


                <h4>Homography</h4>
                <p>
                  To compute the proper homographies between the shirt and the user, we utilized the 10 points the user selected along with the 10 points that were detected from intelligent corner detection and labelling approach. With these 10 sets of correspondences directly mapping with each other, we had enough data to create a proper homography matrix. Lastly, we followed the principles of inverse warping to ensure that there were no leftover “holes” in the final image after warping the shirt onto the user. When completing the inverse warping, we also wanted to exclude any background pixels in our mapping; to do this, we referred to the foreground mask created in a prior step. From here, it was fairly trivial implementing this into our existing homography logic to only warp pixels that are identified as the shirt (foreground) onto our user. This process showed marked improvements over our previous existing approach due to the increased accuracy in background pixel detection. The final outputs of this approach are shown in the results section.
                </p>


        </div>
        <div class="col-sm-12">
            <h2>Experiments and Results</h2>
            <hr>
            <div class="container-fluid">
                <h4>Evaluation Method</h4>
                <p>
                    Throughout the entire development process of this system, we primarily used qualitative analysis to assess weak points and flaws in the approaches we took. We looked for strange extra artifacts in the output, or strange distortions in the results in order to assess the need for a change in approach.<br/><br/>

                    Specifically, while improving our existing homography model, we used human judgement. Typically this is not a very good evaluation metric, because there are no quantitative values to build display progress, however for a result as aesthetic as mapping clothing on a model, we believed this was the best course of action. It was fairly simple to understand that all the white irregular edges around the shirts in approach A were not acceptable. Thus, we were very pleased with the results from approach B where we were able to eliminate practically all of the white noise around the shirt.

                </p>
            </div>
            <div class="container-fluid">
                <h4>Experiments</h4>
                <p><b>Experiment 1</b></p>
                <p>

                    One experiment we attempted was the automatic detection of foreground / background on the user’s
                    uploaded t-shirt images.

                    First, we tried just using the color of the pixel in the top left corner of the image as the
                    background color, and treating any instance of that color as the background. However, we quickly ran
                    into the issue where if the image background was white, then white designs and logos on the t-shirt
                    were also marked are the background. So just using the pixel value would not be enough to
                    effectively identify the background.

                    Next, we tried using sobel edges to detect the edges of the t-shirt and separate it from the
                    background. While the edge detection worked well, it was still hard to figure out how to set all the
                    values inside the shirt to be masked, because the designs on the t-shirt were also outlined by the
                    sobel edge detector. Using a binary_fill_holes method allowed us to highlight the entire area inside
                    the shirt, filling in the areas with patterns and logos. However, the end result was not smooth
                    along the edges, as shown in the images below. With and without a gaussian smoothing filter, the
                    edges of the final result appeared very jagged.
                </p>
                <div class="container-fluid">

                    <img src="assets/images/sobelEdgeDetector.png" class="img-fluid" style="width: 48%">
                    <!--<img src="assets/images/fancy-vest.png" class="img-fluid" style="width: 48%"> -->
                    <p><i>
                        (Sobel Edge Detector with binary_fill_holes applied from SciPy library.)
                    </i></p>
                </div>
                <p>
                    Lastly, we tried first applying a hysteresis threshold to the image. This resulted in a cleaner,
                    smoother edge detection that was also more robust to patterns and designs on the clothing. Applying
                    a sobel edge detector to that result led to smoother results along the edge of the t-shirts.
                    Finally, using the same binary_fill_holes method would fill in the gaps, patterns, and logos on the
                    t-shirt. This sequence of steps resulted in the following smoother foreground masks, as shown below.
                    With these masks, the homography and seam carving functions are more effective at
                    ignoring the background and using the foreground. While the masking results are still not perfect,
                    The use of this mask has significantly improved the quality of the final output.
                </p>
                <div class="container-fluid">

                    <img src="assets/images/hysteresisThreshold.png" class="img-fluid" style="width: 48%">
                    <!--<img src="assets/images/fancy-vest.png" class="img-fluid" style="width: 48%"> -->
                    <p><i>
                        (Results from using Hysteresis Thresholding.)
                    </i></p>
                </div>
                <p>
                    A description of the experiments relating to the intermediate step of resizing the image using
                    modified seam carving can be found in the ‘approach’ section. Those have been omitted here since the
                    results of those experiments drastically changed the algorithm we ended up using and were not
                    representative of the final approach.
                </p>
                <br>

                <p><b>Experiment 2</b></p>

                 <p>
                    In this experiment, we detail our attempts and methods to properly map the clothing of
                    choice onto the user. We used the correspondences that the user selected in the initial setup to
                    create a homography to map our seam-carved clothing onto an image of our user. There were several
                    initial challenges with this approach. The first being that by nature, homographies map the entire
                    image A onto image B. This is particularly problematic because these images often times have
                    backgrounds that definitely should not be included in the final model image with the user.
                </p>
                <div>
                    <img src="assets/images/failedHomography_1.png" class="img-fluid" height="200" width="200">
                    <p><i>(Failed result of using a standard homography because our goal is not to get the entirety of
                        image A onto image B.)</i></p>
                </div>
                <u>Approach A</u>
                <p>
                    An initial heuristic that we employed, similar to what was employed earlier, involved simply taking
                    the pixel in the top left corner and saving that pixel value as the given background color. Now, in
                    the process of doing inverse warpings from the shirt to the user, we will simply conduct a check to
                    ensure that no pixels exactly equal to the background color are mapped over to the final image. We
                    achieved certain levels of success, however clearly this heuristic fails when their are pixels
                    values within
                    the shirt itself that are equal to the background color.
                </p>
                <br>

                <div id="myCarousel5" class="carousel slide" data-ride="carousel" style="width: 50%; margin: auto">
                    <!-- Indicators -->
                    <ol class="carousel-indicators">
                      <li data-target="#myCarousel5" data-slide-to="0" class="active"></li>
                      <li data-target="#myCarousel5" data-slide-to="1"></li>
                      <li data-target="#myCarousel5" data-slide-to="2"></li>
                      <li data-target="#myCarousel5" data-slide-to="3"></li>
                      <li data-target="#myCarousel5" data-slide-to="4"></li>
                      <li data-target="#myCarousel5" data-slide-to="5"></li>
                    </ol>
                    <!-- Wrapper for slides -->
                    <div class="carousel-inner">
                      <div class="item active">
                        <img src="assets/images/homographyNASA.png" alt="Success Case" style="height: 500px; margin: auto">
                      </div>

                      <div class="item">
                        <img src="assets/images/homographyCHAMP.png" alt="Success Case" style="height: 500px; margin: auto">
                      </div>

                      <div class="item">
                        <img src="assets/images/homographyBLUE.png" alt="Failure Case" style="height: 500px; margin: auto">
                      </div>
                        <div class="item">
                            <img src="assets/images/homographySUPERMAN.png" alt="Failure Case" style="height: 500px; margin: auto">
                        </div>
                        <div class="item">
                            <img src="assets/images/homographyNIKE.png" alt="Failure Case" style="height: 500px; margin: auto">
                        </div>
                        <div class="item">
                            <img src="assets/images/homographyAVENGERS.png" alt="Failure Case" style="height: 500px; margin: auto">
                        </div>
                    </div>
                    <!-- Left and right controls -->
                    <a class="left carousel-control" href="#myCarousel5" data-slide="prev">
                      <span class="glyphicon glyphicon-chevron-left"></span>
                      <span class="sr-only">Previous</span>
                    </a>
                    <a class="right carousel-control" href="#myCarousel5" data-slide="next">
                      <span class="glyphicon glyphicon-chevron-right"></span>
                      <span class="sr-only">Next</span>
                    </a>
                  </div>
                  <br>
                <p style="text-align: center;"><i>(Results of homography using approach A.)</i></p>
                <u>Approach B</u>
                <p>
                    To find the most optimal way to warp an image we turned to our experimental work done below (see Experiment 1) with applying the hysteresis thresholding, Sobel Edge Detector, and hole-filling methodology. The output is a 2D boolean array explaining what components are in the foreground versus the background. From here, it was fairly trivial implementing this into our existing homography logic to only warp pixels that are identified as the shirt (foreground) onto our model. This process showed marked improvements over our existing approach due to the increased accuracy in background pixel detection.
                </p>
                <div>
                    <img src="assets/images/homographyApproachB.png" class="img-fluid" height="200" width="200">
                    <img src="assets/images/homographyApproachB2.png" class="img-fluid" height="200" width="200">
                </div>
                <p><i>
                    (Results of homography using approach B.)
                </i></p>

                <b>Experiment 3</b>
                <p>
                  Our current final output homography results in a shirt being properly placed onto the user. Unfortunately, the shirt itself is not perfect before we attach it to the user.There is often a gentle white line outlining the borders of the t-shirt. While minute in detail, this unwanted feature (from grabbing too much of the original shirt in the homography) definitely detracts from the overall appearance of the homography. To help account for this fact, we created an "edge of the shirt" mask. Below are two images displaying the region that we identified as the edge of the image, with varying thicknesses. To calculate the t-shirt edge mask, we took the foreground mask and used a nxn window (where n controlls the width of the edge mask) and checked if all the values in that window in the foreground mask are the same. If there was a mix of foreground / background in that window, then we knew that we had identified an edge, and therefore set all the pixels in that window to True.
                </p>
                <div class="container-fluid">
                    <img src="assets/images/edge_detection_thick.png" class="img-fluid" style="width: 25%">
                    <img src="assets/images/edge_detection_thin.png" class="img-fluid" style="width: 25%">
                    <p><i>(Examples of the edges of the shirt masks prior to convolving a median blurring filter across the edges. The first image uses a 10px border, and the 2nd uses a 25 px border.)</i></p>
                </div>
                <p>
                 Using this mask, we proceeded to apply a blurring filter on the image of the t-shirt, thus blurring the edges. Then, with this t-shirt image with blurred edges, we applied the homography. However, when we applied this technique, it resulted in an unrealistic looking shirt overlaid on the user. This was because the blurring effect lightened the edges of the shirt due to the background, and actually ended up making the background color appear to seep into the rest of the shirt.
                </p>

                <div class="container-fluid">
                    <img src="assets/images/failed_blur.png" class="img-fluid" style="width: 25%">
                    <p><i>(An example final image when the shirt edges are first blurred to try to remove the white outline of the background.)</i></p>
                </div>

                <p>
                  To try blurring the edge of the shirt in a different way, we mapped the t-shirt image onto the user using the foreground mask and saved it into two separate images. Then, we blurred one of the images using a gaussian filter so that there was a blurry version of the target shirt overlaid on top of the user. Next, using the t-shirt edge mask, we went through all pixels marked as edges in the clear output image and replaced the pixels with the blurry corresponding point in the smoothed image. This allowed us to blur the edges of the shirt and get a more realistic looking final result.
                </p>

                <div class="container-fluid">
                    <img src="assets/images/successful_blurredEdges.png" class="img-fluid" style="width: 25%">
                    <p><i>(An example final image when the shirt edges are successfully blurred to hide the white outline of the background. Since we struggled to consistently get the same results, we leave this single very successful case in our experiments section.)</i></p>
                </div>


                <h4>Results of System</h4>
                <p>
                    The images in the slideshow below are the same images shared in the approaches section of this project update. These are the current best outputs we have created using our mapping system.
                </p>
                <br>


                <div id="myCarousel4" class="carousel slide" data-ride="carousel" style="width: 50%; margin: auto">
                    <!-- Indicators -->
                    <ol class="carousel-indicators">
                      <li data-target="#myCarousel4" data-slide-to="0" class="active"></li>
                      <li data-target="#myCarousel4" data-slide-to="1"></li>
                      <li data-target="#myCarousel4" data-slide-to="2"></li>
                      <li data-target="#myCarousel4" data-slide-to="3"></li>
                    </ol>
                    <!-- Wrapper for slides -->
                    <div class="carousel-inner">
                      <div class="item active">
                        <img src="assets/images/black-shirt.png" alt="Success Case" style="height: 500px; margin: auto">
                      </div>

                      <div class="item">
                        <img src="assets/images/homographyApproachB.png" alt="Success Case" style="height: 500px; margin: auto">
                      </div>

                      <div class="item">
                        <img src="assets/images/blue-shirt.jpg" alt="Success Case" style="height: 500px; margin: auto">
                      </div>
                        <div class="item">
                            <img src="assets/images/homographyApproachB2.png" alt="Success Case" style="height: 500px; margin: auto">
                        </div>
                    </div>
                    <!-- Left and right controls -->
                    <a class="left carousel-control" href="#myCarousel4" data-slide="prev">
                      <span class="glyphicon glyphicon-chevron-left"></span>
                      <span class="sr-only">Previous</span>
                    </a>
                    <a class="right carousel-control" href="#myCarousel4" data-slide="next">
                      <span class="glyphicon glyphicon-chevron-right"></span>
                      <span class="sr-only">Next</span>
                    </a>
                </div>
                <br>

                  <p>
                  The next two images show failure cases of our system. Here we see that the polo shirt did not transform well onto our model. This could be due to the fact that the shirt and background were similar in color, which would result in a poor foreground mask and poor corner
                   detection. We also see that trying to fit the green shirt onto the our model failed as well. This failure case could be caused by the same reasons that caused the polo shirt to fail.
                  </p>


                <div id="myCarousel6" class="carousel slide" data-ride="carousel" style="width: 500px; margin: auto;">
                    <!-- Indicators -->
                    <ol class="carousel-indicators">
                      <li data-target="#myCarousel6" data-slide-to="0" class="active"></li>
                      <li data-target="#myCarousel6" data-slide-to="1"></li>
                      <li data-target="#myCarousel6" data-slide-to="2"></li>
                      <li data-target="#myCarousel6" data-slide-to="3"></li>
                    </ol>
                    <!-- Wrapper for slides -->
                    <div class="carousel-inner" style="margin: auto;">
                      <div class="item active" style="margin: auto;">
                        <img src="assets/images/poloShirt.jpg" alt="Success Case" style="height: 500px; margin: auto">
                      </div>

                      <div class="item">
                        <img src="assets/images/poloShirtFail.png" alt="Success Case" style="height: 500px; margin: auto">
                      </div>

                      <div class="item">
                        <img src="assets/images/greenShirt.jpg" alt="Failure Case" style="height: 500px; margin: auto">
                      </div>
                        <div class="item">
                            <img src="assets/images/greenShirtFail.png" alt="Failure Case" style="height: 500px; margin: auto">
                        </div>
                    </div>
                    <!-- Left and right controls -->
                    <a class="left carousel-control" href="#myCarousel6" data-slide="prev">
                      <span class="glyphicon glyphicon-chevron-left"></span>
                      <span class="sr-only">Previous</span>
                    </a>
                    <a class="right carousel-control" href="#myCarousel6" data-slide="next">
                      <span class="glyphicon glyphicon-chevron-right"></span>
                      <span class="sr-only">Next</span>
                    </a>
                </div>



                    <!--<img src="assets/images/homographyApproachB.png" class="img-fluid" height="200" width="200"> -->
                    <!--<img src="assets/images/homographyApproachB2.png" class="img-fluid" height="200" width="200"> -->
        </div>
        <br>
        <div class="col-sm-12">
            <div class="container-fluid">
                <h2>Conclusion & Future Work</h2>
                <hr>
                <p>
                  In our midterm report, we said that for the final project update we wanted to remove the back collar of the shirt from the homographies, remove the need for the user to identify corresponding points on their own uploaded image and in the t-shirt image, and remove the shadowing of the shirt from the homography. All in all, we did a good job on reaching those goals.
                </p>
                <p>
                  We were able to identify the two points of the collar on the shirt and use those points to approximate a circle to describe the neckline and cut that section out of the foreground mask used during the homography. While we were unable to completely remove the need for user input, we were able to automatically identify points on the shirt. Ultimately, this was possible because in our interest point detection, we take advantage of the fact that t-shirt images are taken with very plain, solid color backgrounds. Because of this, we were not able to apply the same logic to the busier-background user-taken image of themselves, and you can imagine that it would be a difficult task to be able to reliably identify and correctly label the 10 interest points on a user-taken image. Additionally, as described in the experiments section, we made good headway on removing the shadowing of the shirt from the homography. We were able to get a single successful case of a blurred t-shirt edge, and would need to invest more time to testing other images to adjust the algorithm we used to do so. The following is a list of more features we would like to include in future iterations of our system:
                </p>
                <li>
                    We still do not have a reliable solution to the issue with the shadows along the edge of the shirt.
                    Not all shirts display this property of having shadows, however for those that do we will need to
                    handle
                    this situation. We hope to apply some form of filtering to blend out these shadows so that the
                    resulting homographies look cleaner. While smoothing the entire shirt can solve the issue of shadows, it limits the system to only work very well on solid colored t-shirts which is not ideal
                </li>
                <li>
                    The resulting homographies are not always clean and 100% on top of the user. As a next step, we want
                    to completely remove user input for selecting the interest points on the images of themselves. We hope to accomplish this by using automatic corner detection while simultaneously
                    finding
                    optimal corresponding points by using these edges. This should theoretically make the homography
                    cleaner and more accurate while removing the painstaking process of having to select
                    correspondences. By further reducing human input we can also increase the convenience of using our tool, therefore making it more attractive to use.
                </li>
                <li>
                    We would like to be able to fit the shirt on a variety of poses, not just the standard one shown.
                    This would allow the user to get a different view than they would normally see.
                </li>
                <li>
                    We would like to make our system more reliable about identifying the corresponding points in certain edge case scenarios. For example, when the shirt is light colored and in a light background, the algorithm for identifying the foreground mask is not very accurate, resulting in poorly identified corresponding points and then also a poor homography. We would like to make our algorithm more robust to these situations so that the user is not limited in the shirts they would like to wear.
                </li>

            </div>
        </div>
        <br>
        <div class="col-sm-12">
            <div class="container-fluid">
                <h2>References</h2>
                <hr>
                <p>

                    [1] Hauswiesner, Stefan et al. “Image-based clothes transfer.” 2011 10th IEEE International
                    Symposium on
                    Mixed and Augmented Reality (2011): 169-172.

                </p>
                <p>
                    [2] Hilsmann A., Eisert P. (2009) Tracking and Retexturing Cloth for Real-Time Virtual Clothing
                    Applications. In: Gagalowicz A., Philips W. (eds) Computer Vision/Computer Graphics
                    CollaborationTechniques. MIRAGE 2009. Lecture Notes in Computer Science, vol 5496. Springer, Berlin,
                    Heidelberg
                </p>
                <p>
                    [3] Avidan, Shai, and Ariel Shamir. “Seam Carving for Content-Aware Image Resizing.” ACM SIGGRAPH
                    2007
                    Papers, ACM, 2007, doi:10.1145/1275808.1276390.
                </p>
            </div>
        </div>
    </div>
</div>


<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
        integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
        crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js"
        integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js"
        integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy"
        crossorigin="anonymous"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
</body>
</html>
